#!/usr/bin/env python3

import argparse
import pathlib
import logging
import datetime
import time
import shutil
import sys
import socket
import copy
import traceback

from dask import delayed, compute
from dask.distributed import Client, LocalCluster, wait, get_worker
from dask_jobqueue import SLURMCluster
import pandas as pd

from deppol_utils import run_and_log, dump_timings


ztf_filtercodes = ['zg', 'zr', 'zi', 'all']
poloka_func = []


import deppol_misc

poloka_func.append({'map': deppol_misc.clean, 'reduce': deppol_misc.clean_reduce})
poloka_func.append({'map': deppol_misc.match_gaia, 'reduce': deppol_misc.match_gaia_reduce})
poloka_func.append({'map': deppol_misc.clean, 'reduce': deppol_misc.clean_reduce})
poloka_func.append({'map': deppol_misc.stats, 'reduce': deppol_misc.stats_reduce})


import deppol_preprocess

poloka_func.append({'map': deppol_preprocess.make_catalog, 'rm': deppol_preprocess.make_catalog_rm})
poloka_func.append({'map': deppol_preprocess.mkcat2, 'rm': deppol_preprocess.mkcat2_rm})
poloka_func.append({'map': deppol_preprocess.makepsf, 'rm': deppol_preprocess.makepsf_rm})
poloka_func.append({'map': deppol_preprocess.pipeline})


import deppol_smphot

poloka_func.append({'reduce': deppol_smphot.reference_quadrant})
poloka_func.append({'reduce': deppol_smphot.smphot})
poloka_func.append({'reduce': deppol_smphot.smphot_plot})


import deppol_astrometry

poloka_func.append({'reduce': deppol_astrometry.wcs_residuals})
poloka_func.append({'reduce': deppol_astrometry.astrometry_fit})


import deppol_photometry

poloka_func.append({'reduce': deppol_photometry.photometry_fit})
poloka_func.append({'reduce': deppol_photometry.photometry_fit_plot})


poloka_func = dict(zip([list(func.values())[0].__name__ for func in poloka_func], poloka_func))


def map_op(results, quadrant, wd, ztfname, filtercode, func, args):
    start_time = time.perf_counter()
    quadrant_path = wd.joinpath("{}/{}/{}".format(ztfname, filtercode, quadrant))

    logger = None
    if func != 'clean':
        logger = logging.getLogger(quadrant)
        logger.setLevel(logging.INFO)

        if args.quadrant_workspace:
            quadrant_workspace = args.quadrant_workspace.joinpath(quadrant)
            shutil.copytree(quadrant_path, quadrant_workspace, symlinks=False, dirs_exist_ok=True)
            quadrant_path = quadrant_workspace

        logger.addHandler(logging.FileHandler(str(quadrant_path.joinpath("output.log")), mode='a'))
        logger.info(datetime.datetime.today())
        logger.info("Running map operation \"{}\" on quadrant {}.".format(func, quadrant))
        logger.info("Quadrant directory: {}".format(quadrant_path))
        logger.info("Working directory: {}".format(wd))

    result = False
    try:
        start_time = time.perf_counter()
        result = poloka_func[func]['map'](quadrant_path, logger, args)
        end_time = time.perf_counter()
    except Exception as e:
        if func != 'clean':
            logger.error(traceback.format_exc())
        print(e)
        print(traceback.format_exc())
    finally:
        if func != 'clean':
            logger.info("End of func \"{}\".".format(func))

            # Remove intermediate files if any
            if args.rm_intermediates and 'rm' in poloka_func[func].keys():
                logger.info("Removing intermediate files...")
                for f in poloka_func[func]['rm']:
                    to_remove = quadrant_path.joinpath(f)
                    if to_remove.exists():
                        to_remove.unlink()
                    else:
                        logger.warning("Tried to remove {} but it does not exist!".format(f))
                        logger.warning(" Full path: {}".format(to_remove))


            if args.dump_timings:
                dump_timings(start_time, end_time, quadrant_dir.joinpath("timings_{}".format(func)))

            if args.quadrant_workspace:
                logger.info("Copying quadrant data from temporary working directory back into original.")
                quadrant_path.joinpath("elixir.fits").unlink()
                shutil.copytree(quadrant_path, wd.joinpath("{}/{}/{}".format(ztfname, filtercode, quadrant)), dirs_exist_ok=True)
                logger.info("Erasing quadrant data from temporary working directory.")
                logger.handlers[0].close() # Needed to flush last msg
                shutil.rmtree(quadrant_path)
            else:
                logger.handlers[0].close()


    return result, time.perf_counter(), start_time, get_worker().id


def reduce_op(results, wd, ztfname, filtercode, func, save_stats, args):
    folder = wd.joinpath("{}/{}".format(ztfname, filtercode))

    # If we want to agregate run statistics on the previous map operation
    if save_stats and results is not None and any(results) and func != 'clean':
        results_df = pd.DataFrame([result for result in results if result is not None], columns=['result', 'time_end', 'time_start', 'worker_id'])
        results_df.to_csv(folder.joinpath("results_{}.csv".format(func)), index=False)

    if not 'reduce' in poloka_func[func].keys():
        return

    logger = logging.getLogger("{}-{}".format(ztfname, filtercode))
    logger.addHandler(logging.FileHandler(folder.joinpath("output.log"), mode='a'))
    logger.setLevel(logging.INFO)
    logger.info(datetime.datetime.today())
    logger.info("Running reduction {}".format(func))

    start_time = time.perf_counter()
    try:
        result = poloka_func[func]['reduce'](folder, ztfname, filtercode, logger, args)
    except Exception as e:
        logger.error("")
        logger.error("In SN {}-{}".format(ztfname, filtercode))
        logger.error(traceback.format_exc())
        print(traceback.format_exc())
    finally:
        pass

    end_time = time.perf_counter()
    if args.dump_timings:
        dump_timings(start_time, end_time, folder.joinpath("timings_{}".format(func)))


if __name__ == '__main__':
    argparser = argparse.ArgumentParser(description="")
    argparser.add_argument('--ztfname', type=pathlib.Path, help="If provided, perform computation on one SN1a. If it points to a valid text file, will perform computation on all keys. If not provided, process the whole working directory.")
    argparser.add_argument('-j', '--n_proc', dest='n_jobs', type=int, default=1)
    argparser.add_argument('--wd', type=pathlib.Path, help="Working directory")
    argparser.add_argument('--filtercode', choices=ztf_filtercodes, default='all', help="Only perform computations on one or all filters.")
    argparser.add_argument('--func', type=str, help="Pipeline function to run. Several functions can be run sequencialy by separating them with commas, eg: \"make_catalog,mkcat2,makepsf\". Available functions: {}".format(list(poloka_func.keys())))
    argparser.add_argument('--no-map', dest='no_map', action='store_true', help="Skip map operations.")
    argparser.add_argument('--no-reduce', dest='no_reduce', action='store_true', help="Skip reduce operations.")
    argparser.add_argument('--cluster-worker', type=int, default=0)
    argparser.add_argument('--scratch', type=pathlib.Path, help="")
    argparser.add_argument('--quadrant-workspace', type=pathlib.Path, help="Quadrant workspace directory to use instead of the one given by --wd. Useful to acceleratre IOs by moving onto a SSD disk or in memory mapped filesystem.")
    argparser.add_argument('--lc-folder', dest='lc_folder', type=pathlib.Path)
    argparser.add_argument('--log-results', action='store_true', default=True)
    argparser.add_argument('--degree', type=int, default=3, help="Degree of ref->quadrant polynomial transformations for relative astrometric.")
    argparser.add_argument('--dump-timings', action='store_true')
    argparser.add_argument('--rm-intermediates', action='store_true', help="Remove intermediate files generated by Poloka.")
    argparser.add_argument('--synchronous-compute', action='store_true', help="Run computation synchronously on the main thread. Usefull for debugging and plotting on the fly.")

    args = argparser.parse_args()
    args.wd = args.wd.expanduser().resolve()

    if args.quadrant_workspace:
        args.quadrant_workspace = args.quadrant_workspace.expanduser().resolve()

    if args.scratch:
        args.scratch = args.scratch.expanduser().resolve()

    filtercodes = ztf_filtercodes[:3]
    if args.filtercode != 'all':
        filtercodes = [args.filtercode]

    # Read ztfnames
    ztfnames = None
    if args.ztfname is not None:
        if args.ztfname.stem == str(args.ztfname):
            ztfnames = [str(args.ztfname)]
        else:
            args.ztfname = args.ztfname.expanduser().resolve()
            if args.ztfname.exists():
                with open(args.ztfname, 'r') as f:
                    ztfnames = [ztfname[:-1] for ztfname in f.readlines()]
            else:
                pass

    print("Found {} SN1a".format(len(ztfnames)))

    # Parse pipeline function
    funcs = args.func.split(",")
    for func in funcs:
        if func not in poloka_func.keys():
            print("\"{}\" func does not exist!".format(func))
            print("Available pipeline funcs:")
            print(list(poloka_func.keys()))
            exit()

    print("Running pipeline:")
    print(" -> ".join(funcs))

    # Temporary folder creation
    if args.quadrant_workspace or args.scratch:
        import signal
        import atexit
        def delete_tree_at_exit(tree_path):
            shutil.rmtree(tree_path, ignore_errors=True)

        if args.quadrant_workspace:
            args.quadrant_workspace.mkdir(exist_ok=True, parents=True)
            atexit.register(delete_tree_at_exit, tree_path=args.quadrant_workspace)

        if args.scratch:
            args.scratch.mkdir(exist_ok=True, parents=True)
            atexit.register(delete_tree_at_exit, tree_path=args.scratch)

    # Allocate cluster
    if args.cluster_worker > 0:
        cluster = SLURMCluster(cores=args.n_jobs,
                               processes=args.n_jobs,
                               memory="{}GB".format(3*args.n_jobs),
                               project="ztf",
                               walltime="12:00:00",
                               queue="htc",
                               job_extra=["-L sps"])

        cluster.scale(jobs=args.cluster_worker)
        client = Client(cluster)
        print(client.dashboard_link, flush=True)
        print(socket.gethostname(), flush=True)
        print("Running {} workers with {} processes each ({} total).".format(args.cluster_worker, args.n_jobs, args.cluster_worker*args.n_jobs))
        client.wait_for_workers(1)
    elif not args.synchronous_compute:
        localCluster = LocalCluster(n_workers=args.n_jobs, dashboard_address='localhost:8787', threads_per_worker=1)
        client = Client(localCluster)

        print("Running a local cluster with {} processes.".format(args.n_jobs))
        print("Dask dashboard at: {}".format(client.dashboard_link))
    else:
        print("Running computations synchronously.")


    jobs = []
    map_count = 0
    reduction_count = 0
    wd = args.wd
    if args.scratch:
        wd = args.scratch

    # Rename compute functions to get better reporting on the dask dashboard
    def _rename_op(op, func):
        _op = op
        _op.__name__ = func
        return _op

    for ztfname in ztfnames:
        for filtercode in filtercodes:
            print("Building job list for {}-{}... ".format(ztfname, filtercode), flush=True, end="")

            band_path = args.wd.joinpath("{}/{}".format(ztfname, filtercode))

            if not band_path.exists():
                print("No quadrant found.")
                continue

            quadrants = list(map(lambda x: x.stem, filter(lambda x: x.is_dir(), band_path.glob("ztf*"))))
            print("{} quadrants found.".format(len(quadrants)))

            results = None
            map_count = 0
            for func in funcs:
                print("Pipeline function \"{}\". ".format(func), flush=True, end="")
                if 'map' in poloka_func[func].keys() and not args.no_map:
                    map_count += len(quadrants)
                    results = [delayed(_rename_op(map_op, func))(results, quadrant, wd, ztfname, filtercode, func, args) for quadrant in quadrants]
                    print("{} map operations. ".format(len(quadrants)), end="", flush=True)

                if ('reduce' in poloka_func[func].keys() or args.log_results) and not args.no_reduce:
                    results = [delayed(_rename_op(reduce_op, func + "_reduce"))(results, wd, ztfname, filtercode, func, True, args)]
                    reduction_count += 1
                    print("Reduction operation.", end="", flush=True)
                    print("")

                if results:
                    jobs.extend(results)

    if args.scratch:
        print("Moving data into scratch folder.")
        for ztfname in ztfnames:
            for filtercode in filtercodes:
                band_path = args.wd.joinpath("{}/{}".format(ztfname, filtercode))
                if band_path.exists():
                    scratch_band_path = args.scratch.joinpath("{}/{}".format(ztfname, filtercode))
                    print("Moving {}-{} into {}".format(ztfname, filtercode, scratch_band_path))
                    shutil.rmtree(scratch_band_path, ignore_errors=True)
                    shutil.copytree(args.wd.joinpath("{}/{}".format(ztfname, filtercode)), scratch_band_path)

    print("")
    print("Running. ", end="")

    if map_count > 0:
        print("Processing {} mappings. ".format(map_count), end="", flush=True)

    if reduction_count > 0:
        print("Processing {} reduction.".format(reduction_count), end="", flush=True)

    print("", flush=True)


    start_time = time.perf_counter()
    if args.synchronous_compute:
        compute(jobs, scheduler="sync")
    else:
        fjobs = client.compute(jobs)
        wait(fjobs)

    print("Done. Elapsed time={}".format(time.perf_counter() - start_time))

    if not args.synchronous_compute:
        client.close()

    if args.scratch:
        print("Moving data back from scratch folder into working directory.")
        print("Scratch folder: {}".format(args.scratch))
        print("Working directory: {}".format(args.wd))
        shutil.copytree(args.scratch, args.wd, dirs_exist_ok=True)
        shutil.rmtree(args.scratch)
